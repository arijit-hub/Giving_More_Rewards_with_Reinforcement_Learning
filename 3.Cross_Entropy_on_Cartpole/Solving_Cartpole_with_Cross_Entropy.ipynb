{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39feaa66",
   "metadata": {},
   "source": [
    "### Balancing Cartpole using Cross Entropy Learning ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfad544",
   "metadata": {},
   "source": [
    "In this project we will build our first smart RL agent which will learn to solve the Cartpole prolem. \n",
    "\n",
    "Before we setup everything, lets get a theoretical sense on what we are going to do in this project. Cross Entropy based RL is a really cool and simple way of solving small problems. What we do is try to make the agent sense some number of episodes (a series of observation and action till it reaches the terminal) and collect the reward. Then what we do is filter out all the episodes with a lower reward value than a threshold. Now we have a nice batch of episodes which have high rewards. Now we give these episodes as trainable instances to a Neural Network and ask it to predict the action to be taken. We attach a Cross Entropy Loss at the end to leverage supervised learning with the actions taken in the episodes. We do it over and over again and finally our NN learns how to balance a Cartpole on a plank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab72dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the necessary libraries ##\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc780d8",
   "metadata": {},
   "source": [
    "Now we have everything in set. \n",
    "\n",
    "What we first do is create our neural network class.\n",
    "\n",
    "It will be a very small FullyConnected Neural Network with two dense layer and the output will have same dimension as the number of actions available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "355cf75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting our NN ##\n",
    "\n",
    "class FullyConnected(nn.Module):\n",
    "    '''\n",
    "    Creates the FC network which acts as the agent.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self , observation_size , hidden_size , action_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(nn.Linear(observation_size , hidden_size) ,  \n",
    "                                     #nn.BatchNorm1d(hidden_size) , \n",
    "                                     nn.ReLU() ,\n",
    "                                     nn.Linear(hidden_size , action_size))\n",
    "        \n",
    "    def forward(self , x):\n",
    "        \n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcfe758",
   "metadata": {},
   "source": [
    "Boom!! We have set up our NN agent.\n",
    "\n",
    "Lets check it if its working correctly!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "665fe853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Output shape is : (32, 8)\n",
      "This must match will the value : (32 , 8)\n"
     ]
    }
   ],
   "source": [
    "## Testing the FullyConnected Class ##\n",
    "\n",
    "test_fc = FullyConnected(observation_size = 4 , hidden_size = 128 , action_size = 8)\n",
    "\n",
    "test_inp = torch.randn((32 , 4))\n",
    "\n",
    "test_out = test_fc(test_inp)\n",
    "print('The Output shape is :' , tuple(test_out.shape))\n",
    "print('This must match will the value : (32 , 8)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249ff9af",
   "metadata": {},
   "source": [
    "Perfect!! Our Neural Network model is made perfectly.\n",
    "\n",
    "Now its time we create an utility function which yields batches of episodes for the NN to train on.\n",
    "\n",
    "We are going to store the episodes in a namedtuple Episodes, which will hold the total reward and the sequence of steps (each step is another named tuple EachEpisodeStep with values observations and actions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d395389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the Episodes and the EachEpisodeStep named tuple ##\n",
    "\n",
    "EachEpisodeStep = namedtuple('EachEpisodeStep' , field_names = ['observation' , 'action'])\n",
    "\n",
    "Episodes = namedtuple('Episodes' , field_names = ['reward' , 'steps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ddab65",
   "metadata": {},
   "source": [
    "With the initial setup, lets create our utility function to yield a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38ca4bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the Datalader utility function ##\n",
    "\n",
    "def dataloader(env , net , batch_size):\n",
    "    \n",
    "    batch = []\n",
    "    total_reward = 0.\n",
    "    episode_step = []\n",
    "    obs = env.reset()\n",
    "    softmax = nn.Softmax(dim = 1)\n",
    "    \n",
    "    while True:\n",
    "        obs_tensor = torch.FloatTensor([obs])\n",
    "        action_prob_tensor = softmax(net(obs_tensor))\n",
    "        action_prob = action_prob_tensor.data.numpy()[0]\n",
    "        \n",
    "        action = np.random.choice(a = len(action_prob) , p = action_prob)\n",
    "        \n",
    "        next_obs , reward , is_done , _ = env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        episode_step.append(EachEpisodeStep(observation = obs , action = action))\n",
    "        \n",
    "        if is_done:\n",
    "            \n",
    "            batch.append(Episodes(reward = total_reward , steps = episode_step))\n",
    "            \n",
    "            total_reward = 0.\n",
    "            \n",
    "            episode_step = []\n",
    "            \n",
    "            next_obs = env.reset()\n",
    "        \n",
    "        if len(batch) == batch_size:\n",
    "            \n",
    "            yield batch\n",
    "            \n",
    "            batch = []\n",
    "            \n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3269eaf6",
   "metadata": {},
   "source": [
    "Now we have created our dataloader, but the problem is we wont be using the entire dataloader batch but just the ones which are above a certain threshold of reward. SO, now we are going to create another utility function which is going to filter the dataloader batch and just send out the episodes which have a reward higher than a threshold. Here we are going to use the percentile functionality to calculate the threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6caaebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filtering utility ##\n",
    "\n",
    "def filter_batch(batch , percentile):\n",
    "    '''\n",
    "    Filters batch and returns only the \n",
    "    datapoints which are in the percentile.\n",
    "    '''\n",
    "    \n",
    "    rewards = list(map(lambda r : r.reward , batch))\n",
    "    \n",
    "    threshold = np.percentile(rewards , percentile)\n",
    "    \n",
    "    mean_reward = float(np.mean(rewards))\n",
    "    \n",
    "    train_observation = []\n",
    "    \n",
    "    train_action = []\n",
    "    \n",
    "    for reward , steps in batch:\n",
    "        \n",
    "        if reward < threshold:\n",
    "            continue\n",
    "            \n",
    "        train_observation.extend(map(lambda step : step.observation , steps))\n",
    "        train_action.extend(map(lambda step : step.action , steps))\n",
    "        \n",
    "    train_observation_tensor = torch.FloatTensor(train_observation)\n",
    "    train_action_tensor = torch.LongTensor(train_action)\n",
    "    \n",
    "    return train_observation_tensor , train_action_tensor , threshold , mean_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e10671",
   "metadata": {},
   "source": [
    "Done!!\n",
    "\n",
    "We are all set to create the final loop to balance our cartpole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4152823b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 : Loss = 0.689 , reward_mean = 21.250 , threshold = 24.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\gym\\envs\\registration.py:565: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "C:\\ProgramData\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:98: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n",
      "C:\\ProgramData\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  1 : Loss = 0.678 , reward_mean = 28.438 , threshold = 33.500\n",
      "Iteration  2 : Loss = 0.660 , reward_mean = 25.812 , threshold = 29.500\n",
      "Iteration  3 : Loss = 0.660 , reward_mean = 34.938 , threshold = 41.500\n",
      "Iteration  4 : Loss = 0.646 , reward_mean = 33.312 , threshold = 32.000\n",
      "Iteration  5 : Loss = 0.653 , reward_mean = 35.875 , threshold = 45.500\n",
      "Iteration  6 : Loss = 0.643 , reward_mean = 38.625 , threshold = 46.000\n",
      "Iteration  7 : Loss = 0.631 , reward_mean = 52.062 , threshold = 58.000\n",
      "Iteration  8 : Loss = 0.629 , reward_mean = 41.125 , threshold = 46.500\n",
      "Iteration  9 : Loss = 0.612 , reward_mean = 54.500 , threshold = 64.500\n",
      "Iteration  10 : Loss = 0.607 , reward_mean = 45.125 , threshold = 57.500\n",
      "Iteration  11 : Loss = 0.614 , reward_mean = 45.062 , threshold = 50.500\n",
      "Iteration  12 : Loss = 0.621 , reward_mean = 55.625 , threshold = 66.000\n",
      "Iteration  13 : Loss = 0.596 , reward_mean = 61.125 , threshold = 69.500\n",
      "Iteration  14 : Loss = 0.592 , reward_mean = 62.562 , threshold = 63.000\n",
      "Iteration  15 : Loss = 0.597 , reward_mean = 53.812 , threshold = 64.500\n",
      "Iteration  16 : Loss = 0.563 , reward_mean = 50.125 , threshold = 54.000\n",
      "Iteration  17 : Loss = 0.593 , reward_mean = 71.375 , threshold = 84.000\n",
      "Iteration  18 : Loss = 0.555 , reward_mean = 64.562 , threshold = 73.000\n",
      "Iteration  19 : Loss = 0.564 , reward_mean = 58.688 , threshold = 59.500\n",
      "Iteration  20 : Loss = 0.573 , reward_mean = 56.812 , threshold = 63.500\n",
      "Iteration  21 : Loss = 0.569 , reward_mean = 69.000 , threshold = 80.500\n",
      "Iteration  22 : Loss = 0.565 , reward_mean = 79.062 , threshold = 82.000\n",
      "Iteration  23 : Loss = 0.543 , reward_mean = 83.062 , threshold = 99.000\n",
      "Iteration  24 : Loss = 0.595 , reward_mean = 91.688 , threshold = 98.000\n",
      "Iteration  25 : Loss = 0.544 , reward_mean = 117.000 , threshold = 143.000\n",
      "Iteration  26 : Loss = 0.558 , reward_mean = 115.938 , threshold = 143.500\n",
      "Iteration  27 : Loss = 0.558 , reward_mean = 107.438 , threshold = 134.500\n",
      "Iteration  28 : Loss = 0.547 , reward_mean = 106.562 , threshold = 108.000\n",
      "Iteration  29 : Loss = 0.537 , reward_mean = 135.938 , threshold = 152.000\n",
      "Iteration  30 : Loss = 0.541 , reward_mean = 143.062 , threshold = 161.000\n",
      "Iteration  31 : Loss = 0.561 , reward_mean = 149.875 , threshold = 156.500\n",
      "Iteration  32 : Loss = 0.540 , reward_mean = 141.062 , threshold = 166.500\n",
      "Iteration  33 : Loss = 0.538 , reward_mean = 136.375 , threshold = 164.000\n",
      "Iteration  34 : Loss = 0.542 , reward_mean = 134.625 , threshold = 159.500\n",
      "Iteration  35 : Loss = 0.545 , reward_mean = 140.500 , threshold = 165.500\n",
      "Iteration  36 : Loss = 0.547 , reward_mean = 146.562 , threshold = 182.500\n",
      "Iteration  37 : Loss = 0.539 , reward_mean = 162.875 , threshold = 185.000\n",
      "Iteration  38 : Loss = 0.534 , reward_mean = 181.500 , threshold = 200.000\n",
      "Iteration  39 : Loss = 0.532 , reward_mean = 181.188 , threshold = 200.000\n",
      "Iteration  40 : Loss = 0.525 , reward_mean = 200.000 , threshold = 200.000\n",
      "Carpole solved!!\n"
     ]
    }
   ],
   "source": [
    "## Creating the agent and the env which will play the entire game ##\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "observation_size = env.observation_space.shape[0]\n",
    "\n",
    "action_size = env.action_space.n\n",
    "\n",
    "net = FullyConnected(observation_size = observation_size , hidden_size = 128 , action_size = action_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optim = torch.optim.Adam(net.parameters() , lr = 1e-2)\n",
    "\n",
    "writer = SummaryWriter(comment = \"-cartpole\")\n",
    "\n",
    "for iter_num , batch in enumerate(dataloader(env , net , 16)):\n",
    "    \n",
    "    train_obs , train_act , threshold , rew_mean = filter_batch(batch , 70)\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    \n",
    "    pred_action = net(train_obs)\n",
    "    \n",
    "    loss = criterion(pred_action , train_act)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optim.step()\n",
    "    \n",
    "    print('Iteration  {} : Loss = {:.3f} , reward_mean = {:.3f} , threshold = {:.3f}'.format(iter_num , \n",
    "                                                                                             loss.item() ,\n",
    "                                                                                             rew_mean , \n",
    "                                                                                             threshold))\n",
    "    \n",
    "    writer.add_scalar('loss' , loss.item() , iter_num)\n",
    "    writer.add_scalar('reward_mean' , rew_mean , iter_num)\n",
    "    writer.add_scalar('threshold' , threshold , iter_num)\n",
    "    \n",
    "    if rew_mean > 199:\n",
    "        print('Carpole solved!!')\n",
    "        break\n",
    "    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40ea54f",
   "metadata": {},
   "source": [
    "Amazing. So we have solved the Cartpole balancing problem with Cross Entropy!!\n",
    "\n",
    "Next up we will go into the details of Q-Learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "rl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
