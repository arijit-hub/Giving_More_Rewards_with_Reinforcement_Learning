{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deb42b3e",
   "metadata": {},
   "source": [
    "### Random Agent and Environment using Python ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3722ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing necessary library ##\n",
    "\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777b412f",
   "metadata": {},
   "source": [
    "In this project we are going to develop a simple environment and an agent without the use of any open source RL library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2c04c2",
   "metadata": {},
   "source": [
    "So, what we will prepare in this project?\n",
    "\n",
    "- [ ] We will build **2 Classes**. The first one is the **Environment Class** and the second one is the **Agent class**.\n",
    "- [ ] In the constructor of the **Environment Class** we will initialize a class variable, *total_step*, with a value of 15. Apart from that The Environment class needs to have the 5 other methods. \n",
    "    - The first is the *observation_space(self)* method, which gives the current observation that the Agent will see in the environment. For our project this will be a list with 3 random values. We don't care actually.\n",
    "    - The second method we need to create is the *action_space(self)* method, which contains the actions an agent can take in the environment. For our project it will be a list with 2 values 0. and 1. .\n",
    "    - Next we will create a *done(self)* method, which will check if our agent has reached the terminal state. For our project it will return True if the class variable total_steps has reduced to 0.\n",
    "    - We also need to create a *reset(self)* method, which resets the environment to the initial state. For our project this would be nothing but to reset the value of the class variable total_steps to 15.\n",
    "    - Finally we will create a method called *do_action(self,action)* which takes in a action from the action_space and returns a reward. For our project it would be simply to reduce the total_steps.\n",
    "- [ ] The **Agent Class** is much simpler than the **Environment Class**. In the constructor, we will initialize the total_reward to zero. We will also create one method called *step(self)* which will take in the environment as its argument. The step method will access the current observation and will choose the current action from the action space. After that it will do the action and gain a reward based on the value of the action and will keep on accumulating it in the total_reward variable.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "942349cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creation of the Environment ##\n",
    "\n",
    "class Environment:\n",
    "    '''\n",
    "    Creates a random environment.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.total_step = 15\n",
    "        \n",
    "        \n",
    "    def observation_space(self):\n",
    "        '''\n",
    "        Returns the current observation space.\n",
    "        '''\n",
    "        \n",
    "        obs_space = []\n",
    "        \n",
    "        for i in range(3):\n",
    "            obs_space.append(round(random.random() , 2))\n",
    "            \n",
    "        return obs_space\n",
    "    \n",
    "    def action_space(self):\n",
    "        '''\n",
    "        Returns the actions available for the agent.\n",
    "        '''\n",
    "        \n",
    "        return [0. , 1.]\n",
    "    \n",
    "    def done(self):\n",
    "        '''\n",
    "        Returns True if the agent has reached the terminal state.\n",
    "        '''\n",
    "        \n",
    "        return self.total_step == 0\n",
    "    \n",
    "    def reset(self):\n",
    "        '''\n",
    "        Resets the environment to its initial state.\n",
    "        '''\n",
    "        \n",
    "        self.total_step = 15\n",
    "        \n",
    "    def do_action(self , action):\n",
    "        '''\n",
    "        Alters the environment by doing an action by the agent.\n",
    "        '''\n",
    "        \n",
    "        if self.done():\n",
    "            raise Exception('Game Over!')\n",
    "            \n",
    "        self.total_step -= 1\n",
    "        \n",
    "        return random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bef320d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building the Agent Class ##\n",
    "\n",
    "class Agent:\n",
    "    '''\n",
    "    Develops a random agent.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.reward = 0.0\n",
    "        \n",
    "    def step(self , environment):\n",
    "        '''\n",
    "        An agent step in the environment.\n",
    "        '''\n",
    "        \n",
    "        observation = environment.observation_space()\n",
    "        \n",
    "        action = random.choice(environment.action_space())\n",
    "        \n",
    "        self.reward += environment.do_action(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da79c20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rewards gained is : 6.007433063018077\n",
      "Total steps is : 15\n"
     ]
    }
   ],
   "source": [
    "## Setting the agent in the wild environment ##\n",
    "\n",
    "env = Environment()\n",
    "\n",
    "agent = Agent()\n",
    "\n",
    "steps_taken = 0\n",
    "\n",
    "while not env.done():\n",
    "    \n",
    "    agent.step(env)\n",
    "    \n",
    "    steps_taken += 1\n",
    "    \n",
    "print('Total rewards gained is :' , agent.reward)\n",
    "print('Total steps is :' , steps_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67851434",
   "metadata": {},
   "source": [
    "Easy breeze. That was one simple start to our RL journey. It was very short but it is indeed very important. From the next notebook we will try to build our agents and environments with other open source RL libraries like OpenAI Gym. Moreover, I would try to develop the theoretical intuition as I go along."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "rl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
