{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2de1767f",
   "metadata": {},
   "source": [
    "### Implementation of the Actor Critic Policy Gradient Algorithm on Cartpole ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21142590",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the necessary packages ##\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ba7004",
   "metadata": {},
   "source": [
    "First off we are going to define out Actor Critic Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e146349",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the actor critic network ##\n",
    "\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    '''\n",
    "    Defines the AC network with a common base and\n",
    "    two heads.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self , obs_dim , action_dim , hidden_layer = 128):\n",
    "        '''\n",
    "        Parameters:\n",
    "        obs_dim : Dimension of the observation space.\n",
    "        action_dim : Dimension of the action space.\n",
    "        hidden_layer : Number of hidden neurons.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base = nn.Sequential(nn.Linear(obs_dim , hidden_layer) ,\n",
    "                                  nn.ReLU() , \n",
    "                                  nn.Linear(hidden_layer , hidden_layer) ,\n",
    "                                  nn.ReLU())\n",
    "        \n",
    "        self.critic = nn.Linear(hidden_layer , 1)\n",
    "        self.actor = nn.Sequential(nn.Linear(hidden_layer , action_dim) ,\n",
    "                                         nn.Softmax())\n",
    "        \n",
    "    def forward(self , obs):\n",
    "        \n",
    "        base_out = self.base(obs)\n",
    "        \n",
    "        state_val = self.critic(base_out)\n",
    "        \n",
    "        action_prob = self.actor(base_out)\n",
    "        \n",
    "        return state_val , action_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e00ae4",
   "metadata": {},
   "source": [
    "With the network defined , we need to set up our Agent class.\n",
    "\n",
    "This is going to be the most important part of this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6280521",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the Agent ##\n",
    "\n",
    "class ACAgent:\n",
    "    '''\n",
    "    Defines the Actor Critic Agent.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self , env , gamma = 0.99 , learning_rate = 1e-5):\n",
    "        \n",
    "        self.env = env\n",
    "        \n",
    "        self.action = None\n",
    "        \n",
    "        self.observation_dim = self.env.observation_space.shape[0] \n",
    "        \n",
    "        self.action_dim = self.env.action_space.n\n",
    "        \n",
    "        self.ac_net = ActorCriticNetwork(self.observation_dim , self.action_dim)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.optim = torch.optim.Adam(self.ac_net.parameters() , lr = self.learning_rate)\n",
    "        \n",
    "    def choose_action(self , observation):\n",
    "        \n",
    "        obs_tensor = torch.Tensor([observation])\n",
    "        \n",
    "        _ , action_prob = self.ac_net(obs_tensor)\n",
    "        \n",
    "        action_distribution = torch.distributions.categorical.Categorical(action_prob)\n",
    "        \n",
    "        action = action_distribution.sample()\n",
    "        \n",
    "        self.action = action\n",
    "        \n",
    "        return action.item()\n",
    "        \n",
    "    def learn(self , state , reward , done , next_state):\n",
    "        \n",
    "        state_tensor = torch.Tensor([state])\n",
    "        reward_tensor = torch.tensor(reward)\n",
    "        done_tensor = torch.Tensor([done]) \n",
    "        next_state_tensor = torch.Tensor([next_state])\n",
    "        \n",
    "        state_value , action_prob = self.ac_net(state_tensor)\n",
    "        \n",
    "        next_state_val , _ = self.ac_net(next_state_tensor)\n",
    "        \n",
    "        delta = reward_tensor + self.gamma * next_state_val * (1 - done) - state_value\n",
    "        \n",
    "        self.optim.zero_grad()\n",
    "        \n",
    "        critic_loss = delta ** 2\n",
    "        \n",
    "        action_distribution = torch.distributions.categorical.Categorical(action_prob)\n",
    "        log_p = action_distribution.log_prob(self.action)\n",
    "        actor_loss = -log_p * delta\n",
    "        \n",
    "        total_loss = actor_loss + critic_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        \n",
    "        self.optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now setting up the environment ##\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "num_step = 1000\n",
    "\n",
    "total_mean_reward = []\n",
    "\n",
    "agent = ACAgent(env)\n",
    "\n",
    "for i in range(num_step):\n",
    "    \n",
    "    episode_reward = []\n",
    "    \n",
    "    obs = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        action = agent.choose_action(obs)\n",
    "        \n",
    "        next_obs , reward , done , _ = env.step(action)\n",
    "        \n",
    "        episode_reward.append(reward)\n",
    "        \n",
    "        agent.learn(obs , reward , done , next_obs)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6257b579",
   "metadata": {},
   "source": [
    "And done!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "rl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
